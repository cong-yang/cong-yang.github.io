<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Cong Yang</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Cong Yang</name>
              </p>
			  <p>I am an Associate Professor at the <a href="http://future.suda.edu.cn/" target="_blank">School of Future Science and Engineering</a> at <a href="http://eng.suda.edu.cn/" target="_blank">Soochow University</a> (SUDA), where I am heading the Ecology and Innovation Center of Intelligent Driving (BeeLab). My work is in the intersection of computer vision, machine learning and autonomous driving, with the goal of making autonomous driving system more robust, reliable and safe in various environments.
              </p>
			  
              <p> 
			  I was a Postdoc researcher at the <a href="https://www.inria.fr/en/teams/magrit" target="_blank">MAGRIT team</a> in <a href="https://www.inria.fr/en/inria-nancy-grand-est-centre" target="_blank">INRIA</a> (France). Later, I worked scientifically on computer vision and machine learning in <a href="https://clobotics.com/" target="_blank">Clobotics</a> and <a href="https://en.horizon.ai/" target="_blank">Horizon Robotics</a>. At Clobotics, I have worked on <a href="https://clobotics.com/retail/" target="_blank">Smart Retail</a> and <a href="https://clobotics.com/wind/" target="_blank">Smart Wind</a> with <a href="https://www.yanke.org/" target="_blank">Dr. Yan Ke</a>. At Horizon Robotics, I led the computer vision team and successfully delivered the intelligent cockpit (Changan <a href="https://en.wikipedia.org/wiki/Changan_UNI-T" target="_blank">UNI-T</a>) based on <a href="https://horizon-robotics-webpage.s3.amazonaws.com/uploads/2020/11/Journey_2_Product_Brief_1.2.pdf" target="_blank">Journey 2 SoC</a>, making UNI-T the world's first massive-produced car using Chinese AI chips. I did my Ph.D. degree in computer vision and pattern recognition from the <a href="http://www.ivg.informatik.uni-siegen.de/en/home" target="_blank">University of Siegen</a> (Germany) in 2016, supervised by <a href="https://www.imi.uni-luebeck.de/~grzegorzek" target="_blank">Prof. Dr. Marcin Grzegorzek</a>.
              </p>
			  
              <p style="text-align:center">
                <a href="mailto:cong.yang@suda.edu.cn">Email</a> &nbsp/&nbsp
                <a href="data/Cong-CV.pdf" target="_blank">CV</a> &nbsp/&nbsp
                <a href="data/Cong-bio.txt" target="_blank">Bio</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=l4tkUxsAAAAJ&hl=en" target="_blank">Google Scholar</a> &nbsp/&nbsp
                <a href="http://web.suda.edu.cn/yangcong/" target="_blank">Chinese Page</a> &nbsp/&nbsp
                <a href="https://github.com/cong-yang" target="_blank">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/cong.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/cong.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research interests include vision-related (also multimodal) perception algorithms in autonomous driving and robotic scenarios, particularly on optimizing algorithms on resource-limited edge devices.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <ul>
				<li>&#10024; February 2025: Our Decoupled OSOD (DOSOD) has been accepted by <a href="https://2025.ieee-icra.org/" target="_blank">International Conference on Robotics and Automation (ICRA 2025)</a>. Congratulations to Yonghao! Source codes are available at <a href="https://github.com/D-Robotics-AI-Lab/DOSOD" target="_blank">GitHub</a>.</li>
				<li>&#10024; September 2024: Our BladeView has been accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8856" target="_blank">IEEE Transactions on Automation Science and Engineering</a></li>
				<li>&#10024; August 2024: Our MoireDet+ and YawnNet have been accepted by <a href="https://icpr2024.org/" target="_blank">International Conference on Pattern Recognition (ICPR 2024)</a>, and <a href="https://icmr2024.org/" target="_blank">International Conference on Multimedia Retrieval (ICMR 2024)</a>, respectively. Congratulations to Zhuochen and Ruoxi!</li>
			    <li>July 2024: Our VRSO has been accepted by <a href="https://iros2024-abudhabi.org/" target="_blank">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2024)</a>, congratulations to Chenyao Yu!</li>
				<li>June 2024: Our RoMe has been accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7274857" target="_blank">IEEE Transactions on Intelligent Vehicles</a>, source codes: <a href="https://github.com/DRosemei/RoMe" target="_blank">GitHub</a>.</li>
				<li>April 2024: I work as an editor in the special issue <a href="https://www.mdpi.com/journal/electronics/special_issues/MXM5ALI072" target="_blank">Recent Advances in Large Language Models</a> in Electronics (IF=3.9). We are looking for your submission! </li>
				<li>April 2024: 1 paper accepted to <a href="https://icmr2024.org/index.html" target="_blank">International Conference on Multimedia Retrieval (ICMR 2024)</a>, source codes of YawnNet will be released soon!</li>
				<li>March 2024: I work as an editor in the research topic <a href="https://www.frontiersin.org/research-topics/51023/efficient-algorithms-for-birds-eye-view-based-perception" target="_blank">Efficient Algorithms for Bird's Eye View-based Perception</a> in Frontiers in Signal Processing. We are looking for your submission! </li>
				<!--<li>January 2024: 1 paper accepted to <a href="https://2024.ieee-icra.org/" target="_blank">IEEE International Conference on Robotics and Automation (ICRA 2024)</a>, source codes of CAMA are available on <a href="https://github.com/manymuch/CAMA" target="_blank">GitHub</a>.</li>-->
			    <!--<li>September 2023: 1 paper accepted to <a href="https://www.springer.com/journal/11263" target="_blank">International Journal of Computer Vision</a> (top journal, IF=19.5), source codes and datasets of SkeView are available on <a href="https://github.com/cong-yang/skeview" target="_blank">GitHub</a>.</li>-->
				<!--<li>August 2023: 1 paper accepted to <a href="https://uobevents.eventsair.com/cikm2023/" target="_blank">CIKM 2023</a> (CORE A), congratulations to Zeyd Boukhers!</li>-->
				<!--<li>July 2023: Source codes of our proposed RoMe (Large-scale Road Surface Reconstruction) is released in <a href="https://github.com/DRosemei/RoMe/tree/main" target="_blank">Github</a>.</li>-->
				<!--<li>December 2022: 1 paper accepted to <a href="https://ieeexplore.ieee.org/document/10006755" target="_blank">IEEE Transactions on Image Processing</a> (top journal, IF=11.041)</li>-->
				<!--<li>December 2022: 1 paper accepted to <a href="https://www.sciencedirect.com/science/article/pii/S0960148122018481" target="_blank">Renewable Energy</a> (top journal, IF=8.634)</li>-->
				<!--<li>October 2022: 1 paper accepted to <a href="https://ieeexplore.ieee.org/document/9931532/" target="_blank">IEEE Transactions on Intelligent Transportation Systems</a> (top journal, IF=9.551)</li>-->
				<!--<li>June 2022: BlumNet is accepted by <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3547816" target="_blank">ACM MM 2022</a></li>-->
              </ul>
              <p>
                
              </p>
            </td>
          </tr>
        </tbody></table>
	

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
              <p>
                For an up-to-date list of my publications, please see my <a href="https://scholar.google.com/citations?user=l4tkUxsAAAAJ&hl=en" target="_blank">Google Scholar profile</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
			 <tr>
				<td style="padding:20px;width:100%;vertical-align:middle">
				  <heading>Teaching</heading>
				  <p>
					<b>Lectures</b>
				  </p>
				  <ul>
					<li><b>2024-Winter</b>: Computer Vision, Soochow University</li>
					<li><b>2024-Winter</b>: Machine Learning, Soochow University</li>
					<li><b>2023-Winter</b>: Computer Vision, Soochow University</li>
					<li><b>2023-Summer</b>: <a href="https://github.com/cong-yang/student_works/tree/main/computer_vision" target="_blank">Computer Vision Practice</a>, Soochow University</li>
					<li><b>2023-Summer</b>: Data Structure, Soochow University</li>
					<li><b>2023-Summer</b>: <a href="https://github.com/cong-yang/student_works/tree/main/data_structure" target="_blank">Data Structure Practice</a>, Soochow University</li>
				  </ul>

				  <p>
					<b>Theses</b><br>
				  </p>
				  <ul>
					<li><b>2023</b>:<br>
					  <i>Shiyuan Chen</i>: Towards Robust Vehicle and Pedestrian Detection via Monocular Camera, Tianjin Polytechnic University<br>
					</li>
					<li><b>2018</b>:<br>
					  <i>Oliver Tiebe</i>: Automatic Skeleton Pruning for Graph-based Object Retrieval, University of Siegen<br>
					</li>
				  </ul>
				</td>
			 </tr>
		</tbody></table>
		
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Datasets and Codes</heading>
            </td>
          </tr>
        </tbody></table>
	
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
			<tr bgcolor="#ffffd0">
				<td style="padding:20px;width:25%;vertical-align:middle">
				  <img src="images/CAMA.png" alt="clean-usnob" width="160" height="160">
				</td>
				<td width="75%" valign="middle">
				  <a href="https://2024.ieee-icra.org/" target="_blank">
					<papertitle>A Vision-Centric Approach for Static Map Element Annotation</papertitle>
				  </a>
				  <br>
				  Jiaxin Zhang, Chen Shiyuan, Haoran Yin, Ruohong Mei, Xuan Liu, <strong>Cong Yang(*)</strong> and Wei Sui 
				  <br>
				  <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2024, pp 1-7.
					<br>
					<a href="https://arxiv.org/abs/2309.11754" target="_blank">paper</a>
					/
					<a href="https://www.youtube.com/watch?v=oBa4ngd2b9Y" target="_blank">video-YouTube</a>
					/
					<a href="https://www.bilibili.com/video/BV1ek4y1F7nJ" target="_blank">video-Bilibili</a>
					/
					<a href="https://github.com/manymuch/CAMA/tree/main" target="_blank">codes-GitHub</a>
					<p></p>
					<p>
				  <p>CAMA: Consistent and Accurate Map Annotation for Intelligent Driving.</p>
				  <p>In use at <a href="https://horizon.cc/" target="_blank">Horizon Robotics</a> for 4D annotation</p>
				</td>
			</tr>
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle">
				  <img src="images/rome.jpg" alt="clean-usnob" width="160" height="160">
				</td>
				<td width="75%" valign="middle">
				  <a href="https://arxiv.org/abs/2306.11368" target="_blank">
					<papertitle>RoMe: Towards Large Scale Road Surface Reconstruction via Mesh Representation</papertitle>
				  </a>
				  <br>
				  Ruohong Mei, Wei Sui, Jiaxin Zhang, Qian Zhang, Tao Peng, 
				  <strong>Cong Yang(*)</strong>
				  <br>
				  <em>arXiv</em>, 2306.11368, 2023, pp 1-7.
					<br>
					<a href="https://arxiv.org/abs/2306.11368" target="_blank">paper</a>
					/
					<a href="https://youtu.be/mTsc_QuOCiE" target="_blank">video-YouTube</a>
					/
					<a href="https://www.bilibili.com/video/BV1Xx4y1o7ea/?vd_source=5051310ed13090afc35ea319bbc5cac3" target="_blank">video-Bilibili</a>
					/
					<a href="https://github.com/DRosemei/RoMe/tree/main" target="_blank">codes</a>
					<p></p>
					<p>
				  <p>A simple yet efficient method, RoMe, for largescale Road surface reconstruction via Mesh representations.</p>
				  <p>In use at <a href="https://horizon.cc/" target="_blank">Horizon Robotics</a> for 4D annotation</p>
				</td>
			</tr>
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle">
				  <img src="images/moiredet.jpg" alt="clean-usnob" width="160" height="160">
				</td>
				<td width="75%" valign="middle">
				  <a href="https://ieeexplore.ieee.org/document/10006755" target="_blank">
					<papertitle>Doing More With Moiré Pattern Detection in Digital Photos</papertitle>
				  </a>
				  <br>
				  <strong>Cong Yang</strong>, 
				  Zhenyu Yang, 
				  <a href="https://www.yanke.org/" target="_blank">Yan Ke</a>, 
				  <a href="http://jdxy.suda.edu.cn/fc/e4/c14015a326884/page.htm" target="_blank">Tao Chen</a>, 
				  <a href="https://www.imi.uni-luebeck.de/~grzegorzek" target="_blank">Marcin Grzegorzek</a>, 
				  <a href="https://johnsee.net/" target="_blank">John See</a>
				  <br>
				  <em>IEEE Transactions on Image Processing</em>, 32, 2023, pp 694-708.
					<br>
					<a href="https://ieeexplore.ieee.org/document/10006755" target="_blank">paper</a>
					/
					<a href="https://github.com/cong-yang/MoireDet/tree/main/demo" target="_blank">video</a>
					/
					<a href="https://github.com/cong-yang/MoireDet/tree/main/MoireDet" target="_blank">codes</a>
					/
					<a href="https://github.com/cong-yang/MoireDet/tree/main/MoireScape" target="_blank">datasets</a>
					<p></p>
					<p>
				  <p>MoireDet algorothm for real-time Moiré Pattern Detection.</p>
				  <p>MoireScape dataset for training and evaluating moiré pattern detection and removal.</p>
				  <p>In use at <a href="https://clobotics.com/retail/" target="_blank">Clobotics Smart Retail</a></p>
				</td>
			</tr>
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle">
				  <img src="images/fatigueview.jpg" alt="clean-usnob" width="160" height="160">
				</td>
				<td width="75%" valign="middle">
				  <a href="https://ieeexplore.ieee.org/document/9931532" target="_blank">
					<papertitle>FatigueView: A Multi-Camera Video Dataset for Vision-based Drowsiness Detection</papertitle>
				  </a>
				  <br>
				  <strong>Cong Yang</strong>, 
				  Zhenyu Yang, 
				  Weiyu Li, 
				  <a href="https://johnsee.net/" target="_blank">John See</a>
				  <br>
				  <em>IEEE Transactions on Intelligent Transportation Systems</em>, 24, 2023, pp 233-246.
					<br>
					<a href="https://ieeexplore.ieee.org/document/9931532" target="_blank">paper</a>
					/
					<a href="https://fatigueview.github.io/#/" target="_blank">project page</a>
					/
					<a href="https://github.com/FatigueView/fatigueview" target="_blank">codes</a>
					/
					<a href="https://github.com/FatigueView/fatigueview/tree/main/dataset" target="_blank">datasets</a>
					<p></p>
					<p>
				  <p>FatigueView is a new large-scale dataset for vision-based drowsiness detection, which is constructed for the research community towards closing the data gap behind the industry.</p>
				  <p>In use at <a href="https://en.wikipedia.org/wiki/Changan_UNI-T" target="_blank">Changan UNI-T</a></p>
				</td>
			</tr>
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle">
				  <img src="images/blade30.jpg" alt="clean-usnob" width="160" height="160">
				</td>
				<td width="75%" valign="middle">
				  <a href="https://www.sciencedirect.com/science/article/pii/S0960148122018481" target="_blank">
					<papertitle>Towards Accurate Image Stitching for Drone-based Wind Turbine Blade Inspection</papertitle>
				  </a>
				  <br>
				  <strong>Cong Yang</strong>, 
				  Xun Liu, 
				  Hua Zhou, 
				  <a href="https://www.yanke.org/" target="_blank">Yan Ke</a>, 
				  <a href="https://johnsee.net/" target="_blank">John See</a>
				  <br>
				  <em>Renewable Energy</em>, 203, 2023, pp 267-279.
					<br>
					<a href="https://www.sciencedirect.com/science/article/pii/S0960148122018481" target="_blank">paper</a>
					/
					<a href="https://github.com/cong-yang/Blade30" target="_blank">datasets</a>
					<p></p>
					<p>
				  <p>Blade30 contains 1,302 real drone-captured images covering 30 full blades captured under various conditions (both on- and off-shore), accompanied by a rich set of annotations such as defects and contaminations, etc.</p>
				  <p>In use at <a href="https://clobotics.com/wind/" target="_blank">Clobotics Smart Wind</a></p>
				</td>
			</tr>
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle">
				  <img src="images/blumnet.jpg" alt="clean-usnob" width="160" height="160">
				</td>
				<td width="75%" valign="middle">
				  <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3547816" target="_blank">
					<papertitle>BlumNet: Graph Component Detection for Object Skeleton Extraction</papertitle>
				  </a>
				  <br>
				  Yulu Zhang, 
				  Liang Sang, 
				  <a href="https://www.imi.uni-luebeck.de/~grzegorzek" target="_blank">Marcin Grzegorzek</a>, 
				  <a href="https://johnsee.net/" target="_blank">John See</a>, 
				  <strong>Cong Yang(*)</strong>			  
				  <br>
				  <em>ACM International Conference on Multimedia</em>, 2022, pp 5527–5536.
					<br>
					<a href="https://dl.acm.org/doi/abs/10.1145/3503161.3547816" target="_blank">paper</a>
					/
					<a href="https://github.com/cong-yang/BlumNet/tree/main/datasets/demo_video" target="_blank">video</a>
					/
					<a href="https://github.com/cong-yang/BlumNet" target="_blank">codes</a>
					<p></p>
					<p>
				  <p>BlumNet is a simple yet efficient framework for extracting object skeletons in natural images and binary shapes. BlumNet has significantly higher accuracy than the state-of-the-art AdaLSN (0.826 vs. 0.786) on the SK1491 dataset, a marked improvement in robustness on mixed object deformations, and also a state-of-the-art performance on binary shape datasets (e.g. 0.893 on the MPEG7 dataset).</p>
				</td>
			</tr>
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle">
				  <img src="images/sitpose.jpg" alt="clean-usnob" width="160" height="160">
				</td>
				<td width="75%" valign="middle">
				  <a href="https://ieeexplore.ieee.org/document/10006755" target="_blank">
					<papertitle>SiTPose: A Siamese Convolutional Transformer for Relative Camera Pose Estimation</papertitle>
				  </a>
				  <br>
				  Kai Leng, 
				  <strong>Cong Yang(*)</strong>,
				  Wei Sui, 
				  Jie Liu, 
				  Zhijun Li (*)
				  <br>
				  <em>IEEE International Conference on Multimedia and Expo (ICME)</em>, 2023.
					<br>
					<a href="pdf/SiTPose-ICME-2023.pdf" target="_blank">paper</a>
					/
					<a href="https://github.com/lktidaohuoxing/SiTPose" target="_blank">codes</a>
					<p></p>
					<p>
				  <p>SiTPose is a siamese convolutional transformer model to regress relative camera pose directly.</p>
				</td>
			</tr>
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle">
				  <img src="images/ego_motion.jpg" alt="clean-usnob" width="160" height="160">
				</td>
				<td width="75%" valign="middle">
				  <a href="https://www.mdpi.com/1424-8220/22/23/9375" target="_blank">
					<papertitle>Towards Accurate Ground Plane Normal Estimation from Ego-Motion</papertitle>
				  </a>
				  <br>
				  Jiaxin Zhang, 
				  Wei Sui, 
				  Qian Zhang, 
				  <a href="http://jdxy.suda.edu.cn/fc/e4/c14015a326884/page.htm" target="_blank">Tao Chen</a>, 
				  <strong>Cong Yang(*)</strong>
				  <br>
				  <em>Sensors</em>, 2022, 22(23), pp 9375.
					<br>
					<a href="https://arxiv.org/abs/2212.04224" target="_blank">arXiv</a>
					/
					<a href="https://github.com/manymuch/ground_normal_filter" target="_blank">codes</a>
					<p></p>
					<p>
				  <p>It uses odometry as input and estimates accurate ground plane normal vectors in real time.</p>
				  <p>In use at <a href="https://horizon.cc/solutions/autonomous-driving/" target="_blank">Horizon Driving Solutions</a></p>
				</td>
			</tr>
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle">
				  <img src="images/piou.jpg" alt="clean-usnob" width="160" height="160">
				</td>
				<td width="75%" valign="middle">
				  <a href="https://arxiv.org/abs/2007.09584" target="_blank">
					<papertitle>PIoU Loss: Towards Accurate Oriented Object Detection in Complex Environments</papertitle>
				  </a>
				  <br>
				  Zhiming Chen, 
				  Kean Chen, 
				  <a href="https://weiyaolin.github.io/" target="_blank">Weiyao Lin(*)</a>,
				  <a href="https://johnsee.net/" target="_blank">John See</a>, 
				  Hui Yu, 
				  <a href="https://www.yanke.org/" target="_blank">Yan Ke</a>, 
				  <strong>Cong Yang(*)</strong>
				  <br>
				  <em>European Conference on Computer Vision</em>, 2020, pp 195-211.
					<br>
					<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500188.pdf" target="_blank">paper</a>
					/
					<a href="https://github.com/clobotics/piou" target="_blank">codes</a>
					<p></p>
					<p>
				  <p>Pixels-IoU (PIoU) Loss is formulated to exploit both the angle and IoU for accurate oriented bounding box (OBB) regression.</p>
				  <p>In use at <a href="https://clobotics.com/retail/" target="_blank">Clobotics Smart Retail</a></p>
				</td>
			</tr>
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle">
				  <img src="images/retail50k.jpg" alt="clean-usnob" width="160" height="160">
				</td>
				<td width="75%" valign="middle">
				  <a href="https://github.com/clobotics/piou/tree/master/retail50k" target="_blank">
					<papertitle>Retail50K dataset</papertitle>
				  </a>
				  <br>
				  <p>Retail50K is a collection of 47,000 images from different supermarkets. Annotations on those images are the layer edges of shelves, fridges and displays, for training and evaluating oriented bounding box (OBB) detectors.</p>
				  <p></p>
				  <a href="dataset/retail50k.zip" target="_blank">datasets download</a>
				</td>
			</tr>
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle">
				  <img src="images/watchpose.jpg" alt="clean-usnob" width="160" height="160">
				</td>
				<td width="75%" valign="middle">
				  <a href="https://github.com/yangcong955/watchpose" target="_blank">
					<papertitle>WatchPose: A View-Aware Approach for Camera Pose Data Collection in Industrial Environments</papertitle>
				  </a>
				  <br>
				  <strong>Cong Yang</strong>, 
				  <a href="https://members.loria.fr/GSimon/" target="_blank">Gilles Simon</a>, 
				  <a href="https://johnsee.net/" target="_blank">John See</a>, 
				  <a href="https://members.loria.fr/moberger/" target="_blank">Marie-Odile Berger</a>, 
				  <a href="https://www.icourses.cn/web/sword/portal/teacherDetails?userId=ff8080813b3c8348013b3fdcd4c9067f" target="_blank">Wenyong Wang</a>
				  <br>
				  <em>Sensors</em>, 2020, 20(11), pp 3045.
					<br>
					<a href="https://www.mdpi.com/1424-8220/20/11/3045" target="_blank">paper</a>
					/
					<a href="https://youtu.be/qbVndYHPiA8" target="_blank">video</a>
					/
					<a href="https://github.com/yangcong955/watchpose" target="_blank">codes</a>
					/
					<a href="https://github.com/yangcong955/watchpose/tree/master/dataset" target="_blank">Industrial10 Dataset</a>
					<p></p>
					<p>
				  <p>WatchPose is a simple yet efficient camera pose data collection method to improve the generalization and robustness of camera pose regression models.</p>
				</td>
			</tr>
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle">
				  <img src="images/contoursegments.jpg" alt="clean-usnob" width="160" height="160">
				</td>
				<td width="75%" valign="middle">
				  <a href="https://link.springer.com/article/10.1007/s00138-017-0823-9" target="_blank">
					<papertitle>Evaluating Contour Segment Descriptors</papertitle>
				  </a>
				  <br>
				  <strong>Cong Yang</strong>, 
				  Oliver Tiebe, 
				  <a href="https://ccilab.doshisha.ac.jp/shirahama/index_en.html" target="_blank">Kimiaki Shirahama</a>, 
				  <a href="https://sin.put.poznan.pl/people/details/ewa.lukasik" target="_blank">Ewa Łukasik</a>, 
				  <a href="https://www.imi.uni-luebeck.de/~grzegorzek" target="_blank">Marcin Grzegorzek</a>
				  <br>
				  <em>Machine Vision and Applications</em>, 28, 2017, pp 373–391.
					<br>
					<a href="pdf/cong-MVA-2017.pdf" target="_blank">paper</a>
					/
					<a href="https://github.com/cong-yang/contour_segment" target="_blank">codes</a>
					<br>
					Datasets:
					<a href="dataset/BerkeleyCS.zip" target="_blank">ETHZ CS</a>
					/
					<a href="dataset/MPEG7CS-Small.zip" target="_blank">MPEG7 CS-small</a>
					/
					<a href="https://github.com/cong-yang/contour_segment/tree/main/datasets" target="_blank">Sketching CS</a>
					<p></p>
				  <p>Source codes of 17 contour segment (CS) descriptors and 4 CS datasets.</p>
				</td>
			</tr>
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle">
				  <img src="images/stripes.jpg" alt="clean-usnob" width="160" height="160">
				</td>
				<td width="75%" valign="middle">
				  <a href="https://link.springer.com/chapter/10.1007/978-3-319-40171-3_5" target="_blank">
					<papertitle>Stripes-based Object Matching</papertitle>
				  </a>
				  <br>
				  Oliver Tiebe, 
				  <strong>Cong Yang(*)</strong>, 
				  Muhammad Hassan Khan, 
				  <a href="https://www.imi.uni-luebeck.de/~grzegorzek" target="_blank">Marcin Grzegorzek</a>, 
				  Dominik Scarpin
				  <br>
				  <em>Computer and Information Science</em>, 656, 2016, pp 59-72.
					<br>
					<a href="pdf/Cong-stripe-2016.pdf" target="_blank">paper</a>
					/
					<a href="https://github.com/cong-yang/stripe_matching" target="_blank">codes</a>
					<p></p>
					<p>
				  <p>A 3D object matching framework based on stripes generated from laser scanning lines.</p>
				</td>
			</tr>
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle">
				  <img src="images/pruning_matching.jpg" alt="clean-usnob" width="160">
				</td>
				<td width="75%" valign="middle">
				  <a href="https://www.amazon.com/Generation-Representation-Matching-Studien-Mustererkennung/dp/3832543996" target="_blank">
					<papertitle>Object Shape Generation, Representation and Matching</papertitle>
				  </a>
				  <br>
				  <strong>Cong Yang</strong>
				  Oliver Tiebe, 
				  <a href="https://ccilab.doshisha.ac.jp/shirahama/index_en.html" target="_blank">Kimiaki Shirahama</a>, 
				  <a href="https://www.imi.uni-luebeck.de/~grzegorzek" target="_blank">Marcin Grzegorzek</a>
				  <br>
				  <em>Pattern Recognition</em>, 55, 2016, pp 183-197.<br>
				  <em>Pattern Recognition Letters</em>, 2016, pp 251-260.
				  <p></p>
				  project page:<br>
					<a href="projects/fine-grained.html" target="_blank">Hierarchical Skeleton</a>
					/
					<a href="projects/interestingpoints.html" target="_blank">High-order Matching</a><br>
					<p></p>
					codes: <br>
					<a href="codes/SkeletonGraph.zip" target="_blank">Skeleton Graph</a>
					/
					<a href="codes/AudioSkeleton.zip" target="_blank">Audio Skeleton</a>
					/
					<a href="codes/PointDetect.zip" target="_blank">Shape Trend</a>
				</td>
			</tr>
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle">
				  <img src="images/skeleton.jpg" alt="clean-usnob" width="160" height="160">
				</td>
				<td width="75%" valign="middle">
				  <a href="https://github.com/cong-yang/skeview" target="_blank">
					<papertitle>Shape and Skeleton-related Codes and Datasets</papertitle>
				  </a>
				  <br>
				  <em>Asian Conference on Computer Vision (ACCV)</em>, 2014, pp 95-110.<br> 
				  <em>International Conference on Multimedia Retrieval (ICMR)</em>, 2015, pp 519-522.<br>
				  <em>International Conference on Pattern Recognition (ICPR)</em>, 2014, pp 3374-3397.
				  <p></p>
				  codes: 
				  <a href="codes/SkeletonPruning.zip" target="_blank">Skeleton Pruning</a>
				  /
				  <a href="codes/SubBox.zip" target="_blank">SubBox</a>
				  /
				  <a href="codes/DCE.zip" target="_blank">DCE Method</a><br>
				  datasets:
					<a href="dataset/mpeg400.zip" target="_blank">MPEG400 Dataset</a>
					/
					<a href="dataset/Tetrapod120.zip" target="_blank">Tetrapod120 Dataset</a>
				</td>
			</tr>
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle">
				  <img src="images/sidiff.jpg" alt="clean-usnob" width="160">
				</td>
				<td width="75%" valign="middle">
				  <a href="https://ieeexplore.ieee.org/document/10006755" target="_blank">
					<papertitle>SiDiff Shape: A search engine for 2D shape matching and retrieval</papertitle>
				  </a>
				  <br>
				  <strong>Cong Yang</strong>, 
				  Oliver Tiebe, 
				  <a href="https://www.researchgate.net/profile/Pit-Pietsch-2" target="_blank">Pit Pietsch</a>, 
				  <a href="http://www.grk1564.uni-siegen.de/de/feinen-christian" target="_blank">Christian Feinen</a>, 
				  <a href="https://www.eti.uni-siegen.de/ws/kmc/members/u_kelter/index.html.en?lang=en" target="_blank">Udo Kelter</a>, 
				  <a href="https://www.imi.uni-luebeck.de/~grzegorzek" target="_blank">Marcin Grzegorzek</a>
				  <br>
				  <em>International Conference on Image Processing (ICIP)</em>, 2014, pp 2202-2206.<br>
					<br>
					<a href="pdf/Cong-ICIP-2014.pdf" target="_blank">paper</a>
					/
					<a href="codes/SkeletonMatching-java.zip" target="_blank">Source Code (Java)</a>
					/
					<a href="documentation/index.html" target="_blank">Documents</a>
				</td>
			</tr>
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle">
				  <img src="images/painting.png" alt="clean-usnob" width="160">
				</td>
				<td width="75%" valign="middle">
				  <a href="https://ieeexplore.ieee.org/document/10006755" target="_blank">
					<papertitle>Source code of KidPating painting tool</papertitle>
				  </a>
				  <br>
				  <em><strong>Cong Yang</strong> @ Imagine Cup 2010</em>
				  <p></p>
					<a href="others/KidSpark.pdf" target="_blank">slides</a> 
					/
					<a href="https://youtu.be/yUZWOp5vXxI" target="_blank">video</a> 
					/
					<a href="others/KidSparkPainting.zip" target="_blank">codes(C#)</a> 
					/
					<a href="https://youtu.be/Cq0VrLQsFqc" target="_blank">hardware</a> 
					/
					<a href="https://youtu.be/YxzltUeWqig" target="_blank">users</a>
				</td>
			</tr>
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle">
				  <img src="images/xmon.png" alt="clean-usnob" width="160">
				</td>
				<td width="75%" valign="middle">
				  <a href="http://cloud.siat.ac.cn/cloud/197/index.html" target="_blank">
					<papertitle>Xmon: A Lightweight Multilayer Open Monitoring Tool for Large-scale Virtual Clusters</papertitle>
				  </a>
				  <br>
				  <strong>Cong Yang</strong>, 
				  Jue Hong, 
				  <a href="https://www.fst.um.edu.mo/people/czxu/" target="_blank">Cheng-Zhong Xu</a>
					<br>
					<a href="pdf/Xmon.pdf" target="_blank">paper</a>
					/
					<a href="https://youtu.be/spN4TqbvFuI" target="_blank">video</a>
					/
					<a href="others/xmon-node-agent.zip" target="_blank">codes</a>
				</td>
			</tr>
			
		</tbody></table>
		
		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Commissions of Trust</heading>
              <p>
                Editor:
              </p>
              <ul>
                <li><b>Frontiers in Signal Processing</b>: <a href="https://www.frontiersin.org/research-topics/51023/efficient-algorithms-for-birds-eye-view-based-perception" target="_blank">2022</a> </li>
              </ul>

              <p>
                Program Committee:
              </p>
              <ul>
                <li><b>International Workshop on Sensor-Based Activity Recognition and Artificial Intelligence</b>: 2023 </li>
              </ul>

              <p>
                Workshop Chair:
              </p>
              <ul>
                <li><b>International Conference on Cybernetics</b>: 2017 </li>
              </ul>

              <p>
                Reviewer:
              </p>
              <ul>
			    <li>ACM Multimedia</li>
				<li>IEEE International Conference on Image Processing</li>
				<li>International Journal of Computer Vision</li>
                <li>IEEE Transactions on Visualization and Computer Graphics</li>
				<li>IEEE Transactions on Industrial Informatics</li>
				<li>IEEE Transactions on Intelligent Transportation Systems</li>
              </ul>
            </td>
          </tr>
        </tbody></table>
   
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                This website is based on the <a href="https://github.com/jonbarron/jonbarron_website" target="_blank">source code</a> of <a href="https://jonbarron.info/" target="_blank">Jon Barron</a>'s website.<br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
